{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "ZRvs0T1o4ToE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "otbKgMeUmdRY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import OneCycleLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DummyTextDataset Class\n",
        "\n",
        "This code defines a custom dataset class `DummyTextDataset` for PyTorch, which generates random text sequences and assigns class labels. It's mainly designed for training purposes in text classification tasks where you need synthetic data. Below is a breakdown of the code:\n",
        "\n",
        "### Class: `DummyTextDataset`\n",
        "\n",
        "#### `__init__(self, num_samples=1000, seq_length=50, vocab_size=1000)`\n",
        "- **Parameters**:\n",
        "  - `num_samples` (default: 1000): The number of text samples in the dataset.\n",
        "  - `seq_length` (default: 50): The length of each text sequence.\n",
        "  - `vocab_size` (default: 1000): The size of the vocabulary for generating text sequences.\n",
        "\n",
        "- **Functionality**:\n",
        "  - Generates `num_samples` random integer sequences representing text data. Each sequence is of length `seq_length` and consists of integers between 0 and `vocab_size-1`.\n",
        "  - The labels are balanced between two classes: class 0 and class 1. Half of the samples are labeled as class 0 and the rest as class 1.\n",
        "  - The data and labels are shuffled randomly to ensure diversity in the dataset.\n",
        "\n",
        "#### `__len__(self)`\n",
        "- Returns the total number of samples in the dataset (i.e., the length of `self.data`).\n",
        "\n",
        "#### `__getitem__(self, idx)`\n",
        "- Fetches the text sequence (`data[idx]`) and its corresponding label (`labels[idx]`) at a given index (`idx`).\n"
      ],
      "metadata": {
        "id": "bFtrT1po4ZUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyTextDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, seq_length=50, vocab_size=1000):\n",
        "        self.data = torch.randint(0, vocab_size, (num_samples, seq_length), dtype=torch.long)\n",
        "\n",
        "        # Ensuring equal number of class 0 and class 1 samples\n",
        "        num_class_0 = num_samples // 2\n",
        "        num_class_1 = num_samples - num_class_0\n",
        "\n",
        "        # Randomly shuffle indices to assign class labels\n",
        "        self.labels = torch.zeros(num_samples, dtype=torch.long)\n",
        "        self.labels[:num_class_0] = 0  # Class 0\n",
        "        self.labels[num_class_0:] = 1  # Class 1\n",
        "\n",
        "        # Shuffle the data and labels together to ensure randomness\n",
        "        indices = torch.randperm(num_samples)\n",
        "        self.data = self.data[indices]\n",
        "        self.labels = self.labels[indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "P9B8BHH0l6WL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LearnablePositionalEncoding Class\n",
        "\n",
        "The `LearnablePositionalEncoding` class implements learnable positional encodings for sequence data in PyTorch. It is particularly useful in transformer models, where the position of elements in the input sequence is crucial but not inherently represented by the model. This class allows the model to learn an optimal representation of sequence positions during training.\n",
        "\n",
        "### Class: `LearnablePositionalEncoding`\n",
        "\n",
        "#### `__init__(self, d_model, max_seq_length)`\n",
        "- **Parameters**:\n",
        "  - `d_model`: The dimension of the model (i.e., the size of the embedding vector for each position).\n",
        "  - `max_seq_length`: The maximum length of the sequence for which positional embeddings will be generated.\n",
        "\n",
        "- **Functionality**:\n",
        "  - Initializes a learnable positional encoding as an `nn.Embedding`, which maps each position in the sequence (up to `max_seq_length`) to a `d_model`-dimensional vector.\n",
        "  - The `positional_encoding` layer creates a lookup table where each position in the sequence (from 0 to `max_seq_length-1`) is assigned a trainable embedding vector.\n",
        "\n",
        "#### `forward(self, x)`\n",
        "- **Input**:\n",
        "  - `x`: A tensor of shape `[batch_size, seq_length, d_model]`, representing the input sequence.\n",
        "  \n",
        "- **Functionality**:\n",
        "  - Extracts the `batch_size` and `seq_length` from the input tensor `x`.\n",
        "  - Creates a tensor of positional indices (from `0` to `seq_length-1`) and replicates it for all items in the batch.\n",
        "  - Uses the `positional_encoding` layer to get a positional embedding for each position in the sequence.\n",
        "  - Adds the positional embeddings to the input `x`, thereby injecting positional information into the model's representation of the input.\n",
        "\n",
        "- **Output**:\n",
        "  - The input `x` with added positional embeddings, ensuring that the sequence order is encoded in the final representation.\n"
      ],
      "metadata": {
        "id": "NaVWVfXP6_og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(LearnablePositionalEncoding, self).__init__()\n",
        "        self.positional_encoding = nn.Embedding(max_seq_length, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x has shape [batch_size, seq_length, d_model]\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Generate positional indices for each position in the sequence\n",
        "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n",
        "\n",
        "        # Pass positional indices through learnable embedding\n",
        "        pos_embeddings = self.positional_encoding(positions)\n",
        "\n",
        "        # Add positional embeddings to input\n",
        "        return x + pos_embeddings"
      ],
      "metadata": {
        "id": "0YtBWNA0oFDG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TextClassificationModel Class\n",
        "\n",
        "The `TextClassificationModel` is a deep learning model built for text classification tasks. It leverages transformer architecture with learnable positional encodings and batch normalization, designed for efficient handling of sequential data. The model takes input sequences, applies embeddings and positional encoding, processes them through a transformer encoder, and outputs a prediction for each input sequence.\n",
        "\n",
        "### Class: `TextClassificationModel`\n",
        "\n",
        "#### `__init__(self, vocab_size, d_model, num_heads, num_layers, max_len, dropout=0.1)`\n",
        "- **Parameters**:\n",
        "  - `vocab_size`: The size of the vocabulary (total number of unique tokens).\n",
        "  - `d_model`: The dimension of the embedding space and the model's internal representations.\n",
        "  - `num_heads`: The number of attention heads in the multi-head self-attention mechanism.\n",
        "  - `num_layers`: The number of transformer encoder layers to stack.\n",
        "  - `max_len`: The maximum length of input sequences.\n",
        "  - `dropout`: Dropout rate for regularization (default is 0.1).\n",
        "  \n",
        "- **Functionality**:\n",
        "  - Initializes an embedding layer that maps each token in the input sequence to a `d_model`-dimensional vector.\n",
        "  - Uses the `LearnablePositionalEncoding` class to generate learnable positional encodings that will be added to the input embeddings, helping the model capture sequential information.\n",
        "  - Creates a transformer encoder with `num_layers` stacked layers, each utilizing multi-head self-attention.\n",
        "  - Includes a batch normalization layer (`nn.BatchNorm1d`) to normalize the activations across the batch for better convergence during training.\n",
        "  - Defines a fully connected layer (`self.fc`) that outputs a prediction for two classes (binary classification).\n",
        "  - The model includes dropout (`self.dropout`) for regularization to prevent overfitting.\n",
        "\n",
        "#### `forward(self, x)`\n",
        "- **Input**:\n",
        "  - `x`: A tensor representing a batch of input sequences with shape `[batch_size, seq_length]`.\n",
        "  \n",
        "- **Functionality**:\n",
        "  - The input sequence `x` is first passed through the embedding layer to convert the tokens into their respective embedding vectors.\n",
        "  - Positional encodings are added to the embeddings to incorporate the sequence order information.\n",
        "  - The embeddings are then permuted to match the expected input shape for the transformer encoder (`[seq_length, batch_size, d_model]`).\n",
        "  - The input is processed by the transformer encoder, which applies multi-head self-attention and feed-forward layers.\n",
        "  - The output is pooled by taking the mean across the sequence length dimension (`dim=0`) to obtain a fixed-length representation for each input sequence.\n",
        "  - Batch normalization is applied to normalize the pooled output, followed by dropout for regularization.\n",
        "  - The final output is passed through a fully connected layer (`fc`), which produces the final class predictions.\n",
        "\n",
        "- **Output**:\n",
        "  - A tensor of shape `[batch_size, 2]`, where each entry corresponds to the model's prediction for two classes (e.g., for binary classification).\n"
      ],
      "metadata": {
        "id": "x46qEzlHYnFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, num_heads, num_layers, max_len, dropout=0.1):\n",
        "    super(TextClassificationModel, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.positional_encoding = LearnablePositionalEncoding(d_model, max_len)\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model, num_heads, dim_feedforward=4*d_model, dropout=dropout, activation='gelu', norm_first=True)\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "    self.bn = nn.BatchNorm1d(d_model)  # Add batch normalization\n",
        "    self.fc = nn.Linear(d_model, 2)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=0)\n",
        "        x = self.bn(x)  # Apply batch normalization\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7-HXBxHEq5OK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation Functions\n",
        "\n",
        "### `train_model` Function\n",
        "\n",
        "The `train_model` function trains a PyTorch model using a given dataloader, optimizer, and loss function. It includes features like gradient clipping, learning rate scheduling, and logging.\n",
        "\n",
        "#### Parameters:\n",
        "- `model`: The model to train (should be a PyTorch neural network model).\n",
        "- `dataloader`: A PyTorch DataLoader that provides the training data in batches.\n",
        "- `batch_size`: The size of the batch for each training step.\n",
        "- `num_epochs`: The number of epochs (iterations over the entire dataset) for training.\n",
        "- `learning_rate`: The learning rate for the optimizer.\n",
        "\n",
        "#### Functionality:\n",
        "1. **Setup**:\n",
        "   - The model is set to training mode using `model.train()`.\n",
        "   - `CrossEntropyLoss` is used as the loss function, which is typical for classification tasks.\n",
        "   - `AdamW` optimizer is used with a weight decay of `0.01` to prevent overfitting.\n",
        "   - A learning rate scheduler (`OneCycleLR`) is set up to adjust the learning rate dynamically during training. The learning rate will warm up for the first 10% of the training steps, then decay.\n",
        "\n",
        "2. **Training Loop**:\n",
        "   - For each epoch:\n",
        "     - The model processes batches from the `dataloader`.\n",
        "     - Gradients are cleared, and predictions are made by the model.\n",
        "     - The loss is calculated using the `CrossEntropyLoss`.\n",
        "     - Gradients are backpropagated and updated using the optimizer.\n",
        "     - Gradient clipping is applied to avoid exploding gradients by limiting the norm of gradients to a maximum value of `1.0`.\n",
        "     - The scheduler is updated to adjust the learning rate based on the training progress.\n",
        "     - Average loss for the epoch is calculated and printed.\n",
        "     - The current learning rate is also printed for monitoring.\n",
        "   \n",
        "3. **Return**:\n",
        "   - The function returns the trained model after all epochs are completed.\n"
      ],
      "metadata": {
        "id": "LMmzAvj0Y1Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, batch_size, num_epochs, learning_rate):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "\n",
        "    # Calculate total steps\n",
        "    total_steps = len(dataloader) * num_epochs\n",
        "\n",
        "    # Create scheduler with warmup\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=learning_rate, total_steps=total_steps, pct_start=0.1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # if batch_idx % 100 == 0:\n",
        "                # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}\")\n",
        "\n",
        "        # Print current learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "8vaCRViQq8YW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Hyperparameters for the model"
      ],
      "metadata": {
        "id": "6fztm0lbY4wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = 1000    # Example vocab size\n",
        "d_model = 256        # Embedding dimension and transformer model dimension\n",
        "num_heads = 8        # Number of attention heads\n",
        "num_layers = 4       # Number of Transformer encoder layers\n",
        "max_len = 50         # Maximum sequence length\n",
        "dropout = 0.1        # Dropout rate\n",
        "batch_size = 32      # Batch size\n",
        "num_epochs = 10      # Number of training epochs\n",
        "learning_rate = 1e-4 # Learning rate"
      ],
      "metadata": {
        "id": "S_ss5REYtTY2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loader"
      ],
      "metadata": {
        "id": "YqHf4y1yY-ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dataset and DataLoader\n",
        "train_dataset = DummyTextDataset(num_samples=1000, seq_length=max_len, vocab_size=vocab_size)\n",
        "test_dataset = DummyTextDataset(num_samples=200, seq_length=max_len, vocab_size=vocab_size)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "4UgwN6Mpq8a5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Definition and Model Training Pipeline"
      ],
      "metadata": {
        "id": "zVVZAXGMZYLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextClassificationModel(vocab_size, d_model, num_heads, num_layers, max_len, dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lct0MHltU-i",
        "outputId": "1f36536a-e2de-4cdc-9141-9143d07b5c68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = train_model(model, train_loader, batch_size, num_epochs, learning_rate)\n",
        "\n",
        "# Evaluate the Model\n",
        "accuracy = evaluate_model(trained_model, test_loader)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bwjqFDrtc2D",
        "outputId": "cdcdc8f6-0215-4b0c-d704-7f9cf1201098"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Average Loss: 0.7417266722768545\n",
            "Current Learning Rate: 9.99970252619065e-05\n",
            "Epoch 2/10, Average Loss: 0.673397159203887\n",
            "Current Learning Rate: 9.679530915668094e-05\n",
            "Epoch 3/10, Average Loss: 0.607156009413302\n",
            "Current Learning Rate: 8.794941226490184e-05\n",
            "Epoch 4/10, Average Loss: 0.5650016283616424\n",
            "Current Learning Rate: 7.452628030325177e-05\n",
            "Epoch 5/10, Average Loss: 0.5031066173687577\n",
            "Current Learning Rate: 5.814494109063477e-05\n",
            "Epoch 6/10, Average Loss: 0.4424118036404252\n",
            "Current Learning Rate: 4.0781225898910744e-05\n",
            "Epoch 7/10, Average Loss: 0.4077887600287795\n",
            "Current Learning Rate: 2.452945504134528e-05\n",
            "Epoch 8/10, Average Loss: 0.36373831517994404\n",
            "Current Learning Rate: 1.1349831933953822e-05\n",
            "Epoch 9/10, Average Loss: 0.3531329296529293\n",
            "Current Learning Rate: 2.8320136340088967e-06\n",
            "Epoch 10/10, Average Loss: 0.3503647171892226\n",
            "Current Learning Rate: 3.374738093512601e-09\n",
            "Test Accuracy: 49.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZohNLM213iN"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}